{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20563dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, messagebox, filedialog\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76be63",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ISHA/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Try to import selenium and BeautifulSoup\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from bs4 import BeautifulSoup\n",
    "    SELENIUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SELENIUM_AVAILABLE = False\n",
    "    print(\"Warning: Selenium/BeautifulSoup not available. Crawler functionality will be limited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c496d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TEXT PREPROCESSING ====================\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has',\n",
    "    'he', 'in', 'is', 'it', 'its', 'of', 'on', 'or', 'that', 'the', 'to',\n",
    "    'was', 'will', 'with', 'this', 'but', 'they', 'have', 'had',\n",
    "    'what', 'when', 'where', 'who', 'why', 'how', 'all', 'each', 'every',\n",
    "    'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n",
    "    'not', 'only', 'same', 'so', 'than', 'too', 'very', 'can', 'just',\n",
    "    'should', 'now'\n",
    "}\n",
    "\n",
    "class TextPreprocessor:\n",
    "    @staticmethod\n",
    "    def preprocess(text):\n",
    "        \"\"\"Convert to lowercase and remove special characters\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        \"\"\"Split text into tokens\"\"\"\n",
    "        return text.split()\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(tokens):\n",
    "        \"\"\"Remove stop words from token list\"\"\"\n",
    "        return [t for t in tokens if t not in STOP_WORDS and len(t) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c168f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INVERTED INDEX ====================\n",
    "\n",
    "class AdvancedInvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list)  # token -> [(doc_id, freq, field)]\n",
    "        self.documents = {}  # doc_id -> full document data\n",
    "        self.doc_count = 0\n",
    "    \n",
    "    def add_document(self, doc_id, doc_data):\n",
    "        \"\"\"Add document to index with all fields searchable\"\"\"\n",
    "        self.documents[doc_id] = doc_data\n",
    "        \n",
    "        # Create searchable text from all fields\n",
    "        searchable_fields = {\n",
    "            'title': doc_data.get('title', ''),\n",
    "            'authors': ' '.join(doc_data.get('authors', [])) if isinstance(doc_data.get('authors', []), list) else str(doc_data.get('authors', '')),\n",
    "            'year': str(doc_data.get('year', '')),\n",
    "            'abstract': doc_data.get('abstract', ''),\n",
    "            'keywords': ' '.join(doc_data.get('keywords', [])) if isinstance(doc_data.get('keywords', []), list) else str(doc_data.get('keywords', ''))\n",
    "        }\n",
    "        \n",
    "        # Index each field with different weights\n",
    "        field_weights = {\n",
    "            'title': 3.0,\n",
    "            'authors': 2.5,\n",
    "            'year': 1.5,\n",
    "            'abstract': 1.0,\n",
    "            'keywords': 2.0\n",
    "        }\n",
    "        \n",
    "        for field, text in searchable_fields.items():\n",
    "            if text:\n",
    "                processed = TextPreprocessor.preprocess(str(text))\n",
    "                tokens = TextPreprocessor.tokenize(processed)\n",
    "                tokens = TextPreprocessor.remove_stopwords(tokens)\n",
    "                \n",
    "                for token in set(tokens):\n",
    "                    freq = tokens.count(token)\n",
    "                    weight = field_weights.get(field, 1.0)\n",
    "                    \n",
    "                    # Check if doc_id already indexed for this token\n",
    "                    existing = [x for x in self.index[token] if x[0] == doc_id]\n",
    "                    if existing:\n",
    "                        idx = self.index[token].index(existing[0])\n",
    "                        doc_id_prev, freq_prev, field_prev = self.index[token][idx]\n",
    "                        self.index[token][idx] = (doc_id, freq_prev + (freq * weight), field)\n",
    "                    else:\n",
    "                        self.index[token].append((doc_id, freq * weight, field))\n",
    "    \n",
    "    def search(self, query):\n",
    "        \"\"\"Advanced search with relevance ranking\"\"\"\n",
    "        processed = TextPreprocessor.preprocess(query)\n",
    "        tokens = TextPreprocessor.tokenize(processed)\n",
    "        original_tokens = tokens.copy()\n",
    "        tokens = TextPreprocessor.remove_stopwords(tokens)\n",
    "        \n",
    "        if not tokens:\n",
    "            return []\n",
    "        \n",
    "        results = defaultdict(float)\n",
    "        term_matches = defaultdict(int)\n",
    "        matched_fields = defaultdict(set)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.index:\n",
    "                idf = math.log(len(self.documents) / len(set(x[0] for x in self.index[token])) + 1)\n",
    "                \n",
    "                for doc_id, freq, field in self.index[token]:\n",
    "                    results[doc_id] += freq * idf\n",
    "                    term_matches[doc_id] += 1\n",
    "                    matched_fields[doc_id].add(field)\n",
    "        \n",
    "        # Boost documents with exact field matches\n",
    "        for doc_id in results:\n",
    "            if 'title' in matched_fields[doc_id]:\n",
    "                results[doc_id] *= 1.5\n",
    "            results[doc_id] += len(matched_fields[doc_id]) * 5\n",
    "        \n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [(doc_id, self.documents[doc_id], score) for doc_id, score in sorted_results if doc_id in self.documents]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save index to file\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'index': dict(self.index),\n",
    "                'documents': self.documents\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load index from file\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.index = defaultdict(list, data['index'])\n",
    "                self.documents = data['documents']\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f07e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== WEB CRAWLER ====================\n",
    "\n",
    "if SELENIUM_AVAILABLE:\n",
    "    class ImprovedSeleniumCrawler:\n",
    "        def __init__(self, callback=None):\n",
    "            self.callback = callback\n",
    "            self.visited_urls = set()\n",
    "            self.publications = []\n",
    "            self.driver = None\n",
    "            self.base_domain = 'pureportal.coventry.ac.uk'\n",
    "        \n",
    "        def log(self, msg):\n",
    "            if self.callback:\n",
    "                self.callback(msg)\n",
    "            print(msg)\n",
    "        \n",
    "        def init_driver(self):\n",
    "            \"\"\"Initialize Selenium WebDriver\"\"\"\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "            chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "            chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "            chrome_options.add_argument('--disable-images')\n",
    "            chrome_options.add_argument('--headless')\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.set_page_load_timeout(20)\n",
    "        \n",
    "        def close_driver(self):\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "        \n",
    "        def crawl_department(self, base_url, max_pages=100):\n",
    "            \"\"\"Crawl department and extract publications\"\"\"\n",
    "            self.log(\"Initializing crawler...\")\n",
    "            self.init_driver()\n",
    "            \n",
    "            try:\n",
    "                self.log(f\"Fetching department page: {base_url}\")\n",
    "                self.driver.get(base_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                \n",
    "                # Extract author links\n",
    "                author_links = self.extract_author_links(soup, base_url)\n",
    "                self.log(f\"Found {len(author_links)} author profiles to crawl\")\n",
    "                \n",
    "                # Crawl each author's profile\n",
    "                for idx, author_link in enumerate(author_links[:max_pages], 1):\n",
    "                    if idx > max_pages:\n",
    "                        break\n",
    "                    \n",
    "                    self.log(f\"\\n[{idx}/{min(len(author_links), max_pages)}] Crawling author: {author_link}\")\n",
    "                    time.sleep(2)  # Polite crawling - 2 second delay\n",
    "                    \n",
    "                    try:\n",
    "                        self.driver.get(author_link)\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                        author_name = self.extract_author_name(soup)\n",
    "                        pubs = self.extract_publications_from_profile(soup, author_link, author_name)\n",
    "                        \n",
    "                        if pubs:\n",
    "                            self.publications.extend(pubs)\n",
    "                            self.log(f\"  ‚úì Extracted {len(pubs)} publications from {author_name}\")\n",
    "                        else:\n",
    "                            self.log(f\"  ‚Üí No publications found on this page\")\n",
    "                    except Exception as e:\n",
    "                        self.log(f\"  ‚úó Error: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                self.log(f\"\\n‚úì Crawling completed. Found {len(self.publications)} total publications\")\n",
    "                return self.publications\n",
    "            \n",
    "            finally:\n",
    "                self.close_driver()\n",
    "        \n",
    "        def extract_author_links(self, soup, base_url):\n",
    "            \"\"\"Extract all person/author profile links\"\"\"\n",
    "            author_links = set()\n",
    "            patterns = [r'/en/persons/[\\w-]+', r'/persons/[\\w-]+']\n",
    "            \n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, href):\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        if self.base_domain in full_url:\n",
    "                            author_links.add(full_url)\n",
    "            \n",
    "            return list(author_links)[:50]\n",
    "        \n",
    "        def extract_author_name(self, soup):\n",
    "            \"\"\"Extract author name from profile page\"\"\"\n",
    "            name_elem = soup.find('h1') or soup.find('h2') or soup.find('span', class_=re.compile('name', re.I))\n",
    "            if name_elem:\n",
    "                return name_elem.get_text(strip=True)\n",
    "            return 'Unknown Author'\n",
    "        \n",
    "        def extract_publications_from_profile(self, soup, profile_link, author_name):\n",
    "            \"\"\"Extract publications from author profile\"\"\"\n",
    "            publications = []\n",
    "            pub_containers = (soup.find_all('article') or \n",
    "                            soup.find_all('div', class_=re.compile('publication', re.I)) or\n",
    "                            soup.find_all('li', class_=re.compile('publication', re.I)))\n",
    "            \n",
    "            for container in pub_containers:\n",
    "                try:\n",
    "                    pub_data = self.parse_publication(container, profile_link, author_name)\n",
    "                    if pub_data and pub_data['title']:\n",
    "                        publications.append(pub_data)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return publications\n",
    "        \n",
    "        def parse_publication(self, container, profile_link, author_name):\n",
    "            \"\"\"Parse individual publication element\"\"\"\n",
    "            pub_data = {\n",
    "                'title': '',\n",
    "                'authors': [],\n",
    "                'year': 'N/A',\n",
    "                'abstract': '',\n",
    "                'keywords': [],\n",
    "                'publication_link': '',\n",
    "                'profile_link': profile_link,\n",
    "                'author_profile_name': author_name,\n",
    "                'crawled_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Extract title\n",
    "            title_elem = container.find(['h3', 'h4', 'h2', 'a'])\n",
    "            if title_elem:\n",
    "                pub_data['title'] = title_elem.get_text(strip=True)\n",
    "            \n",
    "            # Extract year\n",
    "            full_text = container.get_text(' ')\n",
    "            year_match = re.search(r'(19|20)\\d{2}', full_text)\n",
    "            if year_match:\n",
    "                pub_data['year'] = year_match.group()\n",
    "            \n",
    "            # Extract authors\n",
    "            pub_data['authors'] = [author_name]\n",
    "            \n",
    "            # Extract abstract\n",
    "            abstract_elem = container.find(re.compile('abstract|description', re.I))\n",
    "            if abstract_elem:\n",
    "                pub_data['abstract'] = abstract_elem.get_text(strip=True)[:500]\n",
    "            \n",
    "            # Extract publication link\n",
    "            link_elem = container.find('a', href=re.compile(r'/en/publications/', re.I))\n",
    "            if link_elem and link_elem.get('href'):\n",
    "                pub_data['publication_link'] = urljoin(profile_link, link_elem['href'])\n",
    "            \n",
    "            return pub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== K-MEANS CLUSTERING ====================\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, k=3, max_iterations=100):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.vocabulary = set()\n",
    "        self.idf_values = {}\n",
    "        \n",
    "    def preprocess_document(self, text):\n",
    "        \"\"\"Preprocess and tokenize document\"\"\"\n",
    "        processed = TextPreprocessor.preprocess(text)\n",
    "        tokens = TextPreprocessor.tokenize(processed)\n",
    "        tokens = TextPreprocessor.remove_stopwords(tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocabulary(self, documents):\n",
    "        \"\"\"Build vocabulary from all documents\"\"\"\n",
    "        self.vocabulary = set()\n",
    "        for doc in documents:\n",
    "            tokens = self.preprocess_document(doc)\n",
    "            self.vocabulary.update(tokens)\n",
    "        self.vocabulary = sorted(list(self.vocabulary))\n",
    "        \n",
    "    def calculate_idf(self, documents):\n",
    "        \"\"\"Calculate IDF values for terms\"\"\"\n",
    "        doc_count = len(documents)\n",
    "        term_doc_count = defaultdict(int)\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = set(self.preprocess_document(doc))\n",
    "            for token in tokens:\n",
    "                term_doc_count[token] += 1\n",
    "        \n",
    "        for term in self.vocabulary:\n",
    "            self.idf_values[term] = math.log(doc_count / (term_doc_count.get(term, 0) + 1))\n",
    "    \n",
    "    def vectorize_document(self, text):\n",
    "        \"\"\"Convert document to TF-IDF vector\"\"\"\n",
    "        tokens = self.preprocess_document(text)\n",
    "        term_freq = Counter(tokens)\n",
    "        \n",
    "        vector = []\n",
    "        for term in self.vocabulary:\n",
    "            tf = term_freq.get(term, 0)\n",
    "            idf = self.idf_values.get(term, 0)\n",
    "            vector.append(tf * idf)\n",
    "        \n",
    "        return np.array(vector)\n",
    "    \n",
    "    def cosine_similarity(self, v1, v2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        \n",
    "        if norm_v1 == 0 or norm_v2 == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / (norm_v1 * norm_v2)\n",
    "    \n",
    "    def euclidean_distance(self, v1, v2):\n",
    "        \"\"\"Calculate Euclidean distance\"\"\"\n",
    "        return np.linalg.norm(v1 - v2)\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Fit K-means model to documents\"\"\"\n",
    "        # Build vocabulary and calculate IDF\n",
    "        self.build_vocabulary(documents)\n",
    "        self.calculate_idf(documents)\n",
    "        \n",
    "        # Vectorize all documents\n",
    "        vectors = np.array([self.vectorize_document(doc) for doc in documents])\n",
    "        \n",
    "        # Initialize centroids randomly\n",
    "        random_indices = np.random.choice(len(vectors), self.k, replace=False)\n",
    "        self.centroids = vectors[random_indices]\n",
    "        \n",
    "        # K-means iterations\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Assign documents to nearest centroid\n",
    "            clusters = [[] for _ in range(self.k)]\n",
    "            \n",
    "            for idx, vector in enumerate(vectors):\n",
    "                distances = [self.euclidean_distance(vector, centroid) \n",
    "                           for centroid in self.centroids]\n",
    "                cluster_idx = np.argmin(distances)\n",
    "                clusters[cluster_idx].append(idx)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids = []\n",
    "            for cluster in clusters:\n",
    "                if cluster:\n",
    "                    cluster_vectors = vectors[cluster]\n",
    "                    new_centroid = np.mean(cluster_vectors, axis=0)\n",
    "                    new_centroids.append(new_centroid)\n",
    "                else:\n",
    "                    # Keep old centroid if cluster is empty\n",
    "                    new_centroids.append(self.centroids[len(new_centroids)])\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "            \n",
    "            self.centroids = np.array(new_centroids)\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        return clusters\n",
    "    \n",
    "    def predict(self, document):\n",
    "        \"\"\"Predict cluster for a new document\"\"\"\n",
    "        vector = self.vectorize_document(document)\n",
    "        distances = [self.euclidean_distance(vector, centroid) \n",
    "                    for centroid in self.centroids]\n",
    "        return np.argmin(distances)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save clustering model\"\"\"\n",
    "        model_data = {\n",
    "            'k': self.k,\n",
    "            'centroids': self.centroids,\n",
    "            'vocabulary': list(self.vocabulary),\n",
    "            'idf_values': self.idf_values,\n",
    "            'clusters': self.clusters\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load clustering model\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "                self.k = model_data['k']\n",
    "                self.centroids = model_data['centroids']\n",
    "                self.vocabulary = set(model_data['vocabulary'])\n",
    "                self.idf_values = model_data['idf_values']\n",
    "                self.clusters = model_data['clusters']\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1971e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MODERN GUI ====================\n",
    "\n",
    "class ModernSearchEngineGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Coventry University Research Portal\")\n",
    "        self.root.geometry(\"1400x850\")\n",
    "        \n",
    "        # Color scheme\n",
    "        self.colors = {\n",
    "            'primary': '#1a73e8',\n",
    "            'secondary': '#34a853',\n",
    "            'background': '#f8f9fa',\n",
    "            'surface': '#ffffff',\n",
    "            'text': '#202124',\n",
    "            'text_secondary': '#5f6368',\n",
    "            'border': '#dadce0',\n",
    "            'hover': '#e8f0fe',\n",
    "            'accent': '#fbbc04'\n",
    "        }\n",
    "        \n",
    "        self.root.configure(bg=self.colors['background'])\n",
    "        \n",
    "        # Initialize components\n",
    "        self.index = AdvancedInvertedIndex()\n",
    "        self.clustering_model = KMeansClustering(k=3)\n",
    "        self.crawler = None\n",
    "        self.index_file = \"search_index.pkl\"\n",
    "        self.data_file = \"publications.json\"\n",
    "        self.cluster_file = \"clustering_model.pkl\"\n",
    "        self.documents_file = \"clustered_documents.json\"\n",
    "        self.current_results = []\n",
    "        self.clustered_documents = {'Business': [], 'Entertainment': [], 'Health': []}\n",
    "        \n",
    "        self.setup_styles()\n",
    "        self.setup_ui()\n",
    "        self.load_index()\n",
    "        \n",
    "    def setup_styles(self):\n",
    "        \"\"\"Configure custom styles\"\"\"\n",
    "        style = ttk.Style()\n",
    "        style.theme_use('clam')\n",
    "        \n",
    "        style.configure('TFrame', background=self.colors['background'])\n",
    "        style.configure('Surface.TFrame', background=self.colors['surface'])\n",
    "        \n",
    "        style.configure('TNotebook', background=self.colors['background'], borderwidth=0)\n",
    "        style.configure('TNotebook.Tab', padding=[20, 10], font=('Segoe UI', 10),\n",
    "                       background=self.colors['surface'])\n",
    "        style.map('TNotebook.Tab',\n",
    "                 background=[('selected', self.colors['primary'])],\n",
    "                 foreground=[('selected', 'white'), ('!selected', self.colors['text'])])\n",
    "        \n",
    "        style.configure('Primary.TButton', font=('Segoe UI', 10, 'bold'),\n",
    "                       background=self.colors['primary'], foreground='white',\n",
    "                       borderwidth=0, padding=[20, 10])\n",
    "        style.map('Primary.TButton', background=[('active', '#1557b0')])\n",
    "        \n",
    "        style.configure('Secondary.TButton', font=('Segoe UI', 10),\n",
    "                       background=self.colors['surface'], foreground=self.colors['text'],\n",
    "                       borderwidth=1, padding=[15, 8])\n",
    "        \n",
    "        style.configure('Title.TLabel', font=('Segoe UI', 24, 'bold'),\n",
    "                       background=self.colors['surface'], foreground=self.colors['text'])\n",
    "        \n",
    "        style.configure('Subtitle.TLabel', font=('Segoe UI', 11),\n",
    "                       background=self.colors['surface'], foreground=self.colors['text_secondary'])\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Setup main UI\"\"\"\n",
    "        self.create_header()\n",
    "        \n",
    "        content_frame = ttk.Frame(self.root, style='TFrame')\n",
    "        content_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=(0, 20))\n",
    "        \n",
    "        self.notebook = ttk.Notebook(content_frame)\n",
    "        self.notebook.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.create_search_tab()\n",
    "        self.create_crawler_tab()\n",
    "        self.create_clustering_tab()\n",
    "        self.create_stats_tab()\n",
    "    \n",
    "    def create_header(self):\n",
    "        \"\"\"Create header\"\"\"\n",
    "        header = ttk.Frame(self.root, style='Surface.TFrame', height=120)\n",
    "        header.pack(fill=tk.X)\n",
    "        header.pack_propagate(False)\n",
    "        \n",
    "        header_content = ttk.Frame(header, style='Surface.TFrame')\n",
    "        header_content.place(relx=0.5, rely=0.5, anchor=tk.CENTER)\n",
    "        \n",
    "        logo_frame = ttk.Frame(header_content, style='Surface.TFrame', width=60, height=60)\n",
    "        logo_frame.pack(side=tk.LEFT, padx=(0, 15))\n",
    "        logo_frame.pack_propagate(False)\n",
    "        \n",
    "        logo_canvas = tk.Canvas(logo_frame, width=60, height=60, \n",
    "                               bg=self.colors['primary'], highlightthickness=0)\n",
    "        logo_canvas.pack()\n",
    "        logo_canvas.create_text(30, 30, text=\"CU\", font=('Segoe UI', 20, 'bold'), fill='white')\n",
    "        \n",
    "        text_frame = ttk.Frame(header_content, style='Surface.TFrame')\n",
    "        text_frame.pack(side=tk.LEFT)\n",
    "        \n",
    "        ttk.Label(text_frame, text=\"Research Publications Portal\",\n",
    "                 style='Title.TLabel').pack(anchor=tk.W)\n",
    "        ttk.Label(text_frame, text=\"Centre for Computational Science and Mathematical Modelling\",\n",
    "                 style='Subtitle.TLabel').pack(anchor=tk.W, pady=(2, 0))\n",
    "        \n",
    "        separator = ttk.Separator(self.root, orient=tk.HORIZONTAL)\n",
    "        separator.pack(fill=tk.X)\n",
    "    \n",
    "    def create_search_tab(self):\n",
    "        \"\"\"Create search interface\"\"\"\n",
    "        search_frame = ttk.Frame(self.notebook, style='TFrame', padding=20)\n",
    "        self.notebook.add(search_frame, text=\"üîç Search\")\n",
    "        \n",
    "        # Search container\n",
    "        search_container = ttk.Frame(search_frame, style='Surface.TFrame')\n",
    "        search_container.pack(fill=tk.X, pady=(0, 20))\n",
    "        \n",
    "        search_box_frame = ttk.Frame(search_container, style='Surface.TFrame')\n",
    "        search_box_frame.pack(fill=tk.X, padx=20, pady=20)\n",
    "        \n",
    "        ttk.Label(search_box_frame, text=\"Search Publications\",\n",
    "                 font=('Segoe UI', 14, 'bold'), background=self.colors['surface'],\n",
    "                 foreground=self.colors['text']).pack(anchor=tk.W, pady=(0, 10))\n",
    "        \n",
    "        search_input_frame = ttk.Frame(search_box_frame, style='Surface.TFrame')\n",
    "        search_input_frame.pack(fill=tk.X)\n",
    "        \n",
    "        search_icon = ttk.Label(search_input_frame, text=\"üîç\", font=('Segoe UI', 14),\n",
    "                               background='white', foreground=self.colors['text_secondary'])\n",
    "        search_icon.pack(side=tk.LEFT, padx=(5, 0))\n",
    "        \n",
    "        self.search_entry = ttk.Entry(search_input_frame, font=('Segoe UI', 12))\n",
    "        self.search_entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=10, ipady=8)\n",
    "        self.search_entry.bind('<Return>', lambda e: self.perform_search())\n",
    "        \n",
    "        search_btn = ttk.Button(search_input_frame, text=\"Search\",\n",
    "                               style='Primary.TButton', command=self.perform_search)\n",
    "        search_btn.pack(side=tk.LEFT, padx=(0, 5))\n",
    "        \n",
    "        self.results_label = ttk.Label(search_box_frame, text=\"Enter keywords to search publications\",\n",
    "                                      font=('Segoe UI', 9), background=self.colors['surface'],\n",
    "                                      foreground=self.colors['text_secondary'])\n",
    "        self.results_label.pack(anchor=tk.W, pady=(10, 0))\n",
    "        \n",
    "        # Results tree\n",
    "        results_card = ttk.LabelFrame(search_frame, text=\"Search Results\", padding=15)\n",
    "        results_card.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        tree_frame = ttk.Frame(results_card)\n",
    "        tree_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        columns = ('Title', 'Authors', 'Year', 'Relevance')\n",
    "        self.results_tree = ttk.Treeview(tree_frame, columns=columns, \n",
    "                                        show='tree headings', height=12, selectmode='browse')\n",
    "        \n",
    "        self.results_tree.column('#0', width=30, stretch=tk.NO)\n",
    "        self.results_tree.column('Title', width=500, anchor=tk.W)\n",
    "        self.results_tree.column('Authors', width=300, anchor=tk.W)\n",
    "        self.results_tree.column('Year', width=80, anchor=tk.CENTER)\n",
    "        self.results_tree.column('Relevance', width=100, anchor=tk.CENTER)\n",
    "        \n",
    "        self.results_tree.heading('#0', text='#')\n",
    "        self.results_tree.heading('Title', text='üìÑ Title')\n",
    "        self.results_tree.heading('Authors', text='üë§ Authors')\n",
    "        self.results_tree.heading('Year', text='üìÖ Year')\n",
    "        self.results_tree.heading('Relevance', text='‚≠ê Score')\n",
    "        \n",
    "        vsb = ttk.Scrollbar(tree_frame, orient=\"vertical\", command=self.results_tree.yview)\n",
    "        hsb = ttk.Scrollbar(tree_frame, orient=\"horizontal\", command=self.results_tree.xview)\n",
    "        self.results_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)\n",
    "        \n",
    "        self.results_tree.grid(row=0, column=0, sticky='nsew')\n",
    "        vsb.grid(row=0, column=1, sticky='ns')\n",
    "        hsb.grid(row=1, column=0, sticky='ew')\n",
    "        \n",
    "        tree_frame.grid_rowconfigure(0, weight=1)\n",
    "        tree_frame.grid_columnconfigure(0, weight=1)\n",
    "        \n",
    "        self.results_tree.bind('<Double-1>', self.show_publication_details)\n",
    "        self.results_tree.bind('<<TreeviewSelect>>', self.on_result_select)\n",
    "        \n",
    "        # Details panel\n",
    "        details_card = ttk.LabelFrame(search_frame, text=\"Publication Details\", padding=15)\n",
    "        details_card.pack(fill=tk.BOTH, expand=True, pady=(10, 0))\n",
    "        \n",
    "        self.details_text = scrolledtext.ScrolledText(details_card, height=8, wrap=tk.WORD,\n",
    "                                                      font=('Segoe UI', 10), bg='white')\n",
    "        self.details_text.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.details_text.tag_config('title', font=('Segoe UI', 11, 'bold'))\n",
    "        self.details_text.tag_config('label', font=('Segoe UI', 9, 'bold'))\n",
    "        self.details_text.tag_config('link', foreground=self.colors['primary'], underline=True)\n",
    "        \n",
    "        self.details_text.tag_bind('link', '<Button-1>', self.open_link)\n",
    "        self.details_text.tag_bind('link', '<Enter>', \n",
    "                                  lambda e: self.details_text.config(cursor='hand2'))\n",
    "        self.details_text.tag_bind('link', '<Leave>', \n",
    "                                  lambda e: self.details_text.config(cursor=''))\n",
    "        \n",
    "        self.details_text.insert('1.0', 'Select a publication to view details...')\n",
    "        self.details_text.config(state=tk.DISABLED)\n",
    "    \n",
    "\n",
    "    def create_crawler_tab(self):\n",
    "        \"\"\"Create crawler interface\"\"\"\n",
    "        crawler_frame = ttk.Frame(self.notebook, style='TFrame', padding=20)\n",
    "        self.notebook.add(crawler_frame, text=\"üï∑Ô∏è Crawler\")\n",
    "        \n",
    "        settings_card = ttk.LabelFrame(crawler_frame, text=\"Crawler Settings\", padding=20)\n",
    "        settings_card.pack(fill=tk.X, pady=(0, 15))\n",
    "        \n",
    "        ttk.Label(settings_card, text=\"Target URL:\", font=('Segoe UI', 10, 'bold')).pack(anchor=tk.W, pady=(0, 5))\n",
    "        \n",
    "        self.url_entry = ttk.Entry(settings_card, font=('Segoe UI', 10))\n",
    "        self.url_entry.pack(fill=tk.X, ipady=6)\n",
    "        self.url_entry.insert(0, \"https://pureportal.coventry.ac.uk/en/organisations/ics-research-centre-for-computational-science-and-mathematical-mo\")\n",
    "        \n",
    "        max_frame = ttk.Frame(settings_card, style='TFrame')\n",
    "        max_frame.pack(fill=tk.X, pady=(15, 15))\n",
    "        \n",
    "        ttk.Label(max_frame, text=\"Maximum Authors to Crawl:\", font=('Segoe UI', 10, 'bold')).pack(side=tk.LEFT)\n",
    "        \n",
    "        self.max_pages = ttk.Spinbox(max_frame, from_=5, to=500, width=15, font=('Segoe UI', 10))\n",
    "        self.max_pages.set(30)\n",
    "        self.max_pages.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        button_frame = ttk.Frame(settings_card, style='TFrame')\n",
    "        button_frame.pack(fill=tk.X)\n",
    "        \n",
    "        self.crawl_btn = ttk.Button(button_frame, text=\"‚ñ∂ Start Crawling\",\n",
    "                                    style='Primary.TButton', command=self.start_crawling)\n",
    "        self.crawl_btn.pack(side=tk.LEFT, padx=(0, 10))\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"üì¶ Load Sample Data\",\n",
    "                style='Secondary.TButton', command=self.load_sample_data).pack(side=tk.LEFT)\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"üíæ Save Index\",\n",
    "                style='Secondary.TButton', command=self.save_index).pack(side=tk.LEFT, padx=(10, 0))\n",
    "        \n",
    "        # Progress section\n",
    "        progress_card = ttk.LabelFrame(crawler_frame, text=\"Crawling Progress\", padding=20)\n",
    "        progress_card.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.progress_var = tk.DoubleVar()\n",
    "        self.progress_bar = ttk.Progressbar(progress_card, variable=self.progress_var,\n",
    "                                        mode='indeterminate', length=300)\n",
    "        self.progress_bar.pack(fill=tk.X, pady=(0, 10))\n",
    "        \n",
    "        self.status_label = ttk.Label(progress_card, text=\"Ready to crawl\",\n",
    "                                    font=('Segoe UI', 9), \n",
    "                                    foreground=self.colors['text_secondary'])\n",
    "        self.status_label.pack(anchor=tk.W, pady=(0, 10))\n",
    "        \n",
    "        # Log section\n",
    "        log_container = ttk.Frame(progress_card, style='TFrame')\n",
    "        log_container.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.log_text = scrolledtext.ScrolledText(log_container, height=15,\n",
    "                                                font=('Consolas', 9), bg='#f5f5f5',\n",
    "                                                fg=self.colors['text'], relief=tk.FLAT, \n",
    "                                                borderwidth=1)\n",
    "        self.log_text.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Add initial message\n",
    "        self.log_text.insert('1.0', 'Crawler Ready\\n')\n",
    "        self.log_text.insert(tk.END, '='*60 + '\\n')\n",
    "        self.log_text.insert(tk.END, 'Instructions:\\n')\n",
    "        self.log_text.insert(tk.END, '1. Enter target URL or use default\\n')\n",
    "        self.log_text.insert(tk.END, '2. Set maximum authors to crawl (recommended: 20-50)\\n')\n",
    "        self.log_text.insert(tk.END, '3. Click \"Start Crawling\" or \"Load Sample Data\"\\n')\n",
    "        self.log_text.insert(tk.END, '4. Wait for completion and check results in Search tab\\n')\n",
    "        self.log_text.insert(tk.END, '='*60 + '\\n\\n')\n",
    "        self.log_text.config(state=tk.DISABLED)\n",
    "\n",
    "    #Helper method for saving index\n",
    "    def save_index(self):\n",
    "        \"\"\"Save current index to file\"\"\"\n",
    "        try:\n",
    "            self.index.save(self.index_file)\n",
    "            messagebox.showinfo(\"Success\", \"Index saved successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save index: {str(e)}\")\n",
    "\n",
    "    def create_clustering_tab(self):\n",
    "        \"\"\"Create document clustering interface\"\"\"\n",
    "        cluster_frame = ttk.Frame(self.notebook, style='TFrame', padding=20)\n",
    "        self.notebook.add(cluster_frame, text=\"üìä Clustering\")\n",
    "        \n",
    "        # Training section\n",
    "        train_card = ttk.LabelFrame(cluster_frame, text=\"Train Clustering Model\", padding=20)\n",
    "        train_card.pack(fill=tk.X, pady=(0, 15))\n",
    "        \n",
    "        ttk.Label(train_card, text=\"Load documents for clustering (Business, Entertainment, Health)\",\n",
    "                 font=('Segoe UI', 10)).pack(anchor=tk.W, pady=(0, 10))\n",
    "        \n",
    "        btn_frame = ttk.Frame(train_card)\n",
    "        btn_frame.pack(fill=tk.X)\n",
    "        \n",
    "        ttk.Button(btn_frame, text=\"üìÅ Load Documents\", style='Secondary.TButton',\n",
    "                  command=self.load_documents_for_clustering).pack(side=tk.LEFT, padx=(0, 10))\n",
    "        ttk.Button(btn_frame, text=\"üéØ Train Model\", style='Primary.TButton',\n",
    "                  command=self.train_clustering_model).pack(side=tk.LEFT)\n",
    "        \n",
    "        self.cluster_status = ttk.Label(train_card, text=\"No model trained\",\n",
    "                                       font=('Segoe UI', 9), foreground=self.colors['text_secondary'])\n",
    "        self.cluster_status.pack(anchor=tk.W, pady=(10, 0))\n",
    "\n",
    "        # Prediction section\n",
    "        predict_card = ttk.LabelFrame(cluster_frame, text=\"Classify New Document\", padding=20)\n",
    "        predict_card.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        ttk.Label(predict_card, text=\"Enter document text:\",\n",
    "                 font=('Segoe UI', 10, 'bold')).pack(anchor=tk.W, pady=(0, 5))\n",
    "        \n",
    "        self.cluster_input = scrolledtext.ScrolledText(predict_card, height=6, \n",
    "                                                       font=('Segoe UI', 10), wrap=tk.WORD)\n",
    "        self.cluster_input.pack(fill=tk.BOTH, expand=True, pady=(0, 10))\n",
    "        \n",
    "        ttk.Button(predict_card, text=\"üîÆ Predict Cluster\", style='Primary.TButton',\n",
    "                  command=self.predict_cluster).pack(anchor=tk.W)\n",
    "        \n",
    "        self.cluster_result = ttk.Label(predict_card, text=\"\",\n",
    "                                       font=('Segoe UI', 12, 'bold'),\n",
    "                                       foreground=self.colors['primary'])\n",
    "        self.cluster_result.pack(anchor=tk.W, pady=(10, 0))\n",
    "\n",
    "    def create_stats_tab(self):\n",
    "        \"\"\"Create statistics tab\"\"\"\n",
    "        stats_frame = ttk.Frame(self.notebook, style='TFrame', padding=20)\n",
    "        self.notebook.add(stats_frame, text=\"üìà Statistics\")\n",
    "        \n",
    "        quick_stats = ttk.Frame(stats_frame, style='TFrame')\n",
    "        quick_stats.pack(fill=tk.X, pady=(0, 20))\n",
    "        \n",
    "        self.stat_pubs = self.create_stat_card(quick_stats, \"üìö\", \"Total Publications\", \"0\", 0)\n",
    "        self.stat_authors = self.create_stat_card(quick_stats, \"üë•\", \"Unique Authors\", \"0\", 1)\n",
    "        self.stat_terms = self.create_stat_card(quick_stats, \"üî§\", \"Indexed Terms\", \"0\", 2)\n",
    "        \n",
    "        # Additional stats\n",
    "        info_card = ttk.LabelFrame(stats_frame, text=\"Index Information\", padding=20)\n",
    "        info_card.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        self.stats_text = scrolledtext.ScrolledText(info_card, height=15,\n",
    "                                                    font=('Consolas', 9), wrap=tk.WORD)\n",
    "        self.stats_text.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        ttk.Button(stats_frame, text=\"üîÑ Refresh Statistics\", style='Secondary.TButton',\n",
    "                  command=self.update_statistics).pack(pady=(10, 0))\n",
    "        \n",
    "    def create_stat_card(self, parent, icon, label, value, col):\n",
    "        \"\"\"Create statistics card\"\"\"\n",
    "        card = ttk.Frame(parent, style='Surface.TFrame', relief=tk.RAISED, borderwidth=1)\n",
    "        card.grid(row=0, column=col, padx=10, sticky='ew')\n",
    "        parent.grid_columnconfigure(col, weight=1)\n",
    "        \n",
    "        content = ttk.Frame(card, style='Surface.TFrame')\n",
    "        content.pack(padx=20, pady=20)\n",
    "        \n",
    "        ttk.Label(content, text=icon, font=('Segoe UI', 32),\n",
    "                 background=self.colors['surface']).pack()\n",
    "        \n",
    "        value_label = ttk.Label(content, text=value, font=('Segoe UI', 24, 'bold'),\n",
    "                               background=self.colors['surface'], foreground=self.colors['primary'])\n",
    "        value_label.pack()\n",
    "        \n",
    "        ttk.Label(content, text=label, font=('Segoe UI', 10),\n",
    "                 background=self.colors['surface'], foreground=self.colors['text_secondary']).pack()\n",
    "        \n",
    "        return value_label\n",
    "    \n",
    "    # Search functionality\n",
    "    def perform_search(self):\n",
    "        \"\"\"Execute search query\"\"\"\n",
    "        query = self.search_entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Empty Query\", \"Please enter a search query\")\n",
    "            return\n",
    "        \n",
    "        results = self.index.search(query)\n",
    "        self.current_results = results\n",
    "        \n",
    "        # Clear tree\n",
    "        for item in self.results_tree.get_children():\n",
    "            self.results_tree.delete(item)\n",
    "        \n",
    "        # Populate results\n",
    "        for idx, (doc_id, doc_data, score) in enumerate(results, 1):\n",
    "            authors = doc_data.get('authors', [])\n",
    "            if isinstance(authors, list):\n",
    "                authors_str = ', '.join(authors[:3])\n",
    "            else:\n",
    "                authors_str = str(authors)\n",
    "            if len(authors) > 3:\n",
    "                authors_str += f\" +{len(authors)-3} more\"\n",
    "            \n",
    "            self.results_tree.insert('', tk.END, text=str(idx),\n",
    "                                    values=(doc_data.get('title', 'N/A'),\n",
    "                                           authors_str,\n",
    "                                           doc_data.get('year', 'N/A'),\n",
    "                                           f\"{score:.2f}\"))\n",
    "        \n",
    "        self.results_label.config(text=f\"Found {len(results)} result(s) for '{query}'\")\n",
    "\n",
    "    def on_result_select(self, event):\n",
    "        \"\"\"Handle result selection\"\"\"\n",
    "        selection = self.results_tree.selection()\n",
    "        if not selection:\n",
    "            return\n",
    "        \n",
    "        item = self.results_tree.item(selection[0])\n",
    "        row_num = int(item['text']) - 1\n",
    "        \n",
    "        if 0 <= row_num < len(self.current_results):\n",
    "            _, doc_data, _ = self.current_results[row_num]\n",
    "            self.display_details(doc_data)\n",
    "\n",
    "    def display_details(self, doc_data):\n",
    "        \"\"\"Display publication details\"\"\"\n",
    "        self.details_text.config(state=tk.NORMAL)\n",
    "        self.details_text.delete('1.0', tk.END)\n",
    "        \n",
    "        # Title\n",
    "        self.details_text.insert(tk.END, doc_data.get('title', 'N/A'), 'title')\n",
    "        self.details_text.insert(tk.END, '\\n\\n')\n",
    "        \n",
    "        # Authors\n",
    "        self.details_text.insert(tk.END, 'üë§ Authors: ', 'label')\n",
    "        authors = doc_data.get('authors', [])\n",
    "        if isinstance(authors, list):\n",
    "            self.details_text.insert(tk.END, ', '.join(authors) + '\\n\\n')\n",
    "        else:\n",
    "            self.details_text.insert(tk.END, str(authors) + '\\n\\n')\n",
    "        \n",
    "        # Year\n",
    "        self.details_text.insert(tk.END, 'üìÖ Year: ', 'label')\n",
    "        self.details_text.insert(tk.END, str(doc_data.get('year', 'N/A')) + '\\n\\n')\n",
    "\n",
    "        # Abstract\n",
    "        abstract = doc_data.get('abstract', '')\n",
    "        if abstract:\n",
    "            self.details_text.insert(tk.END, 'üìù Abstract: ', 'label')\n",
    "            self.details_text.insert(tk.END, abstract + '\\n\\n')\n",
    "        \n",
    "        # Links\n",
    "        pub_link = doc_data.get('publication_link', '')\n",
    "        if pub_link:\n",
    "            self.details_text.insert(tk.END, 'üîó Publication: ', 'label')\n",
    "            self.details_text.insert(tk.END, pub_link, ('link', pub_link))\n",
    "            self.details_text.insert(tk.END, '\\n\\n')\n",
    "        \n",
    "        profile_link = doc_data.get('profile_link', '')\n",
    "        if profile_link:\n",
    "            self.details_text.insert(tk.END, 'üë§ Profile: ', 'label')\n",
    "            self.details_text.insert(tk.END, profile_link, ('link', profile_link))\n",
    "        \n",
    "        self.details_text.config(state=tk.DISABLED)\n",
    "\n",
    "    def open_link(self, event):\n",
    "        \"\"\"Open clicked link\"\"\"\n",
    "        index = self.details_text.index(tk.CURRENT)\n",
    "        tags = self.details_text.tag_names(index)\n",
    "        \n",
    "        for tag in tags:\n",
    "            if tag.startswith('http'):\n",
    "                webbrowser.open(tag)\n",
    "                break\n",
    "    \n",
    "    def show_publication_details(self, event):\n",
    "        \"\"\"Show details on double-click\"\"\"\n",
    "        pass  # Already handled by selection\n",
    "\n",
    "    # Crawler functionality\n",
    "    def start_crawling(self):\n",
    "        \"\"\"Start web crawling\"\"\"\n",
    "        if not SELENIUM_AVAILABLE:\n",
    "            messagebox.showerror(\"Error\", \"Selenium is not installed. Please install selenium and beautifulsoup4.\")\n",
    "            return\n",
    "        \n",
    "        url = self.url_entry.get().strip()\n",
    "        if not url:\n",
    "            messagebox.showwarning(\"Empty URL\", \"Please enter a target URL\")\n",
    "            return\n",
    "        \n",
    "        max_pages = int(self.max_pages.get())\n",
    "        \n",
    "        self.crawl_btn.config(state=tk.DISABLED)\n",
    "        self.progress_bar.start()\n",
    "        self.log_text.delete('1.0', tk.END)\n",
    "        \n",
    "        def crawl_thread():\n",
    "            try:\n",
    "                crawler = ImprovedSeleniumCrawler(callback=self.log_message)\n",
    "                publications = crawler.crawl_department(url, max_pages)\n",
    "                \n",
    "                # Save publications\n",
    "                with open(self.data_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(publications, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                # Build index\n",
    "                self.index = AdvancedInvertedIndex()\n",
    "                for idx, pub in enumerate(publications):\n",
    "                    self.index.add_document(f\"pub_{idx}\", pub)\n",
    "                \n",
    "                self.index.save(self.index_file)\n",
    "                \n",
    "                self.root.after(0, lambda: messagebox.showinfo(\"Success\", \n",
    "                    f\"Crawled {len(publications)} publications successfully!\"))\n",
    "                self.root.after(0, self.update_statistics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.root.after(0, lambda: messagebox.showerror(\"Error\", str(e)))\n",
    "            finally:\n",
    "                self.root.after(0, lambda: self.crawl_btn.config(state=tk.NORMAL))\n",
    "                self.root.after(0, self.progress_bar.stop)\n",
    "        \n",
    "        threading.Thread(target=crawl_thread, daemon=True).start()\n",
    "    \n",
    "    def log_message(self, msg):\n",
    "        \"\"\"Log crawler message\"\"\"\n",
    "        def update():\n",
    "            self.log_text.insert(tk.END, msg + '\\n')\n",
    "            self.log_text.see(tk.END)\n",
    "            self.status_label.config(text=msg[:100])\n",
    "        \n",
    "        self.root.after(0, update)\n",
    "    \n",
    "    def load_sample_data(self):\n",
    "        \"\"\"Load sample data\"\"\"\n",
    "        sample_data = [\n",
    "            {\n",
    "                'title': 'Machine Learning Applications in Healthcare',\n",
    "                'authors': ['Dr. John Smith', 'Prof. Jane Doe'],\n",
    "                'year': '2023',\n",
    "                'abstract': 'This paper explores machine learning applications in medical diagnosis...',\n",
    "                'keywords': ['machine learning', 'healthcare', 'AI'],\n",
    "                'publication_link': 'https://example.com/pub1',\n",
    "                'profile_link': 'https://example.com/profile1'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Computational Modeling of Climate Systems',\n",
    "                'authors': ['Prof. Alice Johnson'],\n",
    "                'year': '2022',\n",
    "                'abstract': 'A comprehensive study on climate modeling using computational methods...',\n",
    "                'keywords': ['climate', 'modeling', 'simulation'],\n",
    "                'publication_link': 'https://example.com/pub2',\n",
    "                'profile_link': 'https://example.com/profile2'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        with open(self.data_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_data, f, indent=2)\n",
    "        \n",
    "        self.index = AdvancedInvertedIndex()\n",
    "        for idx, pub in enumerate(sample_data):\n",
    "            self.index.add_document(f\"pub_{idx}\", pub)\n",
    "        \n",
    "        self.index.save(self.index_file)\n",
    "        self.update_statistics()\n",
    "        messagebox.showinfo(\"Success\", \"Sample data loaded successfully!\")\n",
    "    \n",
    "    # Clustering functionality\n",
    "    def load_documents_for_clustering(self):\n",
    "        \"\"\"Load documents for clustering\"\"\"\n",
    "        filepath = filedialog.askopenfilename(\n",
    "            title=\"Select Documents File\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if not filepath:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                self.clustered_documents = json.load(f)\n",
    "            \n",
    "            total = sum(len(docs) for docs in self.clustered_documents.values())\n",
    "            self.cluster_status.config(\n",
    "                text=f\"Loaded {total} documents across {len(self.clustered_documents)} categories\"\n",
    "            )\n",
    "            messagebox.showinfo(\"Success\", f\"Loaded {total} documents successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load documents: {str(e)}\")\n",
    "    \n",
    "    def train_clustering_model(self):\n",
    "        \"\"\"Train K-means clustering model\"\"\"\n",
    "        all_docs = []\n",
    "        for category, docs in self.clustered_documents.items():\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        if len(all_docs) < 10:\n",
    "            messagebox.showwarning(\"Insufficient Data\", \n",
    "                \"Please load at least 10 documents for training\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.clustering_model = KMeansClustering(k=3)\n",
    "            self.clustering_model.fit(all_docs)\n",
    "            self.clustering_model.save_model(self.cluster_file)\n",
    "            \n",
    "            self.cluster_status.config(\n",
    "                text=f\"Model trained on {len(all_docs)} documents with 3 clusters\",\n",
    "                foreground=self.colors['secondary']\n",
    "            )\n",
    "            messagebox.showinfo(\"Success\", \"Clustering model trained successfully!\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Training failed: {str(e)}\")\n",
    "    \n",
    "    def predict_cluster(self):\n",
    "        \"\"\"Predict cluster for input text\"\"\"\n",
    "        text = self.cluster_input.get('1.0', tk.END).strip()\n",
    "        \n",
    "        if not text:\n",
    "            messagebox.showwarning(\"Empty Input\", \"Please enter some text\")\n",
    "            return\n",
    "        \n",
    "        if self.clustering_model.centroids is None:\n",
    "            messagebox.showwarning(\"No Model\", \"Please train the model first\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            cluster_id = self.clustering_model.predict(text)\n",
    "            cluster_names = ['Business', 'Entertainment', 'Health']\n",
    "            cluster_name = cluster_names[cluster_id] if cluster_id < len(cluster_names) else f\"Cluster {cluster_id}\"\n",
    "            \n",
    "            self.cluster_result.config(text=f\"‚úì Predicted Category: {cluster_name}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "    \n",
    "    # Utility functions\n",
    "    def load_index(self):\n",
    "        \"\"\"Load saved index\"\"\"\n",
    "        if os.path.exists(self.index_file):\n",
    "            if self.index.load(self.index_file):\n",
    "                self.update_statistics()\n",
    "    \n",
    "    def update_statistics(self):\n",
    "        \"\"\"Update statistics display\"\"\"\n",
    "        num_docs = len(self.index.documents)\n",
    "        num_terms = len(self.index.index)\n",
    "        \n",
    "        # Count unique authors\n",
    "        all_authors = set()\n",
    "        for doc in self.index.documents.values():\n",
    "            authors = doc.get('authors', [])\n",
    "            if isinstance(authors, list):\n",
    "                all_authors.update(authors)\n",
    "            else:\n",
    "                all_authors.add(str(authors))\n",
    "        \n",
    "        self.stat_pubs.config(text=str(num_docs))\n",
    "        self.stat_authors.config(text=str(len(all_authors)))\n",
    "        self.stat_terms.config(text=str(num_terms))\n",
    "        \n",
    "        # Detailed stats\n",
    "        self.stats_text.delete('1.0', tk.END)\n",
    "        self.stats_text.insert(tk.END, f\"Total Publications: {num_docs}\\n\")\n",
    "        self.stats_text.insert(tk.END, f\"Unique Authors: {len(all_authors)}\\n\")\n",
    "        self.stats_text.insert(tk.END, f\"Indexed Terms: {num_terms}\\n\\n\")\n",
    "        \n",
    "        if num_docs > 0:\n",
    "            years = [doc.get('year', 'N/A') for doc in self.index.documents.values()]\n",
    "            year_counts = Counter(years)\n",
    "            \n",
    "            self.stats_text.insert(tk.END, \"Publications by Year:\\n\")\n",
    "            for year, count in sorted(year_counts.items(), reverse=True)[:10]:\n",
    "                self.stats_text.insert(tk.END, f\"  {year}: {count}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cca50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ MAIN ====================\n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    app = ModernSearchEngineGUI(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
